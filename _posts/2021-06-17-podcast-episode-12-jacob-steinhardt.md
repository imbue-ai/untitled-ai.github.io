---
layout: post
title:  "Generally Intelligent #12: Jacob Steinhardt, UC Berkeley, on machine learning safety, alignment and measurement "
date:   2021-06-17 09:00:00 -0700
category: machine-learning
tags: podcast 

author: 
- <a href="https://twitter.com/kanjun">Kanjun Qiu </a>
- <a href="http://joshalbrecht.com/">Josh Albrecht</a> (<a href="mailto:joshalbrecht@gmail.com">email</a>)

---

**[RSS](https://anchor.fm/s/42cab330/podcast/rss)** Â· **[Spotify](https://open.spotify.com/show/1hikWa5LWDQJwXtz5LoeVn)** Â· **[Apple Podcasts](https://podcasts.apple.com/us/podcast/generally-intelligent/id1544921720)** Â· **[Pocket Casts](https://pca.st/ewh266dr)** 

<iframe src="https://anchor.fm/untitled-ai/embed/episodes/Episode-12-Jacob-Steinhardt--UC-Berkeley--on-machine-learning-safety--alignment-and-measurement-e12vsjf" width="100%" frameborder="0" scrolling="no"></iframe>

<br>

Jacob Steinhardt [(Google Scholar)](https://scholar.google.com/citations?user=LKv32bgAAAAJ&hl=en) [(Website)](https://jsteinhardt.stat.berkeley.edu/) is an assistant professor at UC Berkeley.  His main research interest is in designing machine learning systems that are reliable and aligned with human values.  Some of his specific research directions include robustness, rewards specification and reward hacking, as well as scalable alignment. 

His most recent [paper](https://arxiv.org/abs/2009.03300) at ICLR 2021 proposes a new test to measure an NLP modelâ€™s accuracy on a wide variety of tasks, ranging from mathematics, US history, law, and more.  It provides a measurement tool to help researchers specify an important problem: while current models can achieve superhuman performance on benchmarks, they lack the ability to understand language on a whole.  Another of Jacobâ€™s [papers](https://arxiv.org/abs/2008.02275) at ICLR focuses on measuring a language modelâ€™s knowledge of basic concepts of morality.  It shows that current language models have a promising but incomplete ability to predict basic human ethical judgements.

**Highlights from our conversation:**

ğŸ“œ â€œTest accuracy is a very limited metric.â€œ

ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ â€œYou might not be able to get lots of feedback on human values.â€

ğŸ“Š â€Iâ€™m interested in measuring the progress in AI capabilities.â€


<br>
*The full transcript is coming soon. Below are the show notes. As always, please [feel free to reach out](https://twitter.com/kanjun) with feedback, ideas, and questions!*
<!--more-->
<br />

### Some quotes we loved
> **[11:33] On the freedom of knowing how to communicate unusual ideas:** 
â€œBut I had to learn how to write a good paper without having a template.  I think it required me to learn to become a significantly better writer. And I think that helped later on, because it made me feel more comfortable pursuing unusual ideas. I knew I had the skills to present those ideas.  As long as I believed in them, I could get other people to believe in them.â€
>
> **[34:55] On learning hard phenomena from big data sets:**
â€œPeople have historically been interested in these parts, like compositionality of objects and occlusion...but thinking about these complicated things directly is just not really the right way to go. You just want this very diverse distribution of things that are deeply ingrained in evolutionary history as opposed to being part of explicit reasoningâ€
>
> **[21:10] Why measurements matters for AI safety:**
â€œI've been really obsessed with this idea of measurement. First of all, test accuracy is a very limited metric. What are we trying to do with it?  I'm kind of thinking in analogy with climate change as another field.  For a while, there was a lot of climate skepticism or climate denial.  At some point it becomes pretty clear, when there's regular heat waves fires and that sort of thing.  You probably wanted to do something about it before that point.  Having these more subtle measurements that you can look at are important. And the other thing is I think it actually laid the groundwork for the more extreme weather events to become a convincing signal.

<br>

### Show Notes
* Jacobâ€™s original career plan and the exploration that led him to ML [01:40]
* Jacobâ€™s first research area in grad school: computationally bounded reasoning [05:47]
* Pivoting to robustness research [09:16]
* How Jacobâ€™s early research directions helped him learn to communicate unusual ideas [10:40]
* Two different types of adversarial robustness research and its significance [12:30]
* Gap year at Open Philanthropy and OpenAI [17:11]
* Learning about scaling at OpenAI [18:08]
* Working on Covid research and lessons [19:37]
* Working on measurement (not just test accuracy but other metrics) [21:09]
* Measuring AI capability jumps for safety [24:23]
* Measuring progress in AI capabilities - Jacobâ€™s  [paper](https://arxiv.org/abs/2008.02275)  [27:14]
* AI calibration for prediction accuracy [27:55]
* A summary of different types of robustness [29:16]
* Dan Hendryckâ€™s work with collecting data sets [30:39]
* Jacobâ€™s most unusual  [paper](https://arxiv.org/abs/2008.02275) , measuring the ethics of AI models [33:19]
* What work has impacted Jacob most? (GPT-3, scaling laws for NNs) [36:23]
* Why does measurement matter? And Jacobâ€™s interest in the history of science. [38:37]
* Filtering your information diet [40:17]
* Should researchers be hedgehogs or foxes? [42:25]
* What methods are underrated in ML research? [46:34]
* Jacobâ€™s  [paper](https://arxiv.org/abs/1807.03341)  on troubling trends in Machine Learning Scholarship [47:49]
* Attributes of a great research lab [53:02]
* What makes for a great advisor? [56:07]
* What makes for a great researcher? [57:21]

<br>

*Thanks to <a href="https://twitter.com/lukelivesfree">Luke Cheng </a> for writing drafts of this post and <a href="https://www.linkedin.com/in/tessajhall/">Tessa Hall</a> for editing the podcast.*
